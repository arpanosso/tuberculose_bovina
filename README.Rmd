---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Tuberculo Bovina

### Carregando os pacotes

```{r, warning=FALSE, error=FALSE, message=FALSE}
library(tidymodels)
library(ISLR)
library(tidyverse)
library(modeldata)
library(pROC)
library(vip)
library(readxl)
```

### Entrada de dados
```{r}
tbsp <- read_excel("data-raw/TBSP_outubro.xls") |> 
  mutate(TB = forcats::as_factor(TB))
glimpse(tbsp)
tbsp |> count(TB)
```


### Definindo a base de treino e a base de teste

```{r}
set.seed(1)
tbsp_initial_split <- initial_split(tbsp, strata = "TB", prop = 0.75)

tbsp_train <- training(tbsp_initial_split)
tbsp_train |> count(TB)
tbsp_test  <- testing(tbsp_initial_split)
tbsp_test |> count(TB)
```


### Análise exploratória dos dados

```{r}
skimr::skim(tbsp_train)
visdat::vis_miss(tbsp_train)
tbsp_train  |> 
   select(where(is.numeric))  |> 
   cor()  |> 
   corrplot::corrplot()
```

# Regressão Logística (não é Machine Learning...)

## Data Prep

```{r}
tbsp_recipe <- recipe(TB ~ ., data = tbsp_train |> 
                        select(-id, -cod_mun)
                      ) |> 
  step_normalize(all_numeric_predictors())  |> 
  step_novel(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  # step_poly(c(n_vacas, prod_diaria), degree = 9)  |> 
  step_dummy(all_nominal_predictors())

bake(prep(tbsp_recipe), new_data = NULL)
visdat::vis_miss(bake(prep(tbsp_recipe), new_data = NULL))
```


### Modelo  

#### Definição de

a) a f(x): logistc_reg()

b) modo (natureza da var resp): classification

c) hiperparametros a tunar: penalty = tune()

d) hiperparametros  não tunado: mixture = 1 # LASSO

e) o motor que queremos usar: glmnet


```{r}
tbsp_lr_model <- logistic_reg(penalty = tune(), mixture = 1)  |> 
  set_mode("classification") |> 
  set_engine("glmnet")
```
  
# Workflow

```{r}
tbsp_lr_wf <- workflow()  |> 
  add_model(tbsp_lr_model) |> 
  add_recipe(tbsp_recipe)
```

## Tunagem (tune)

a) bases de reamostragem para validação: vfold_cv()

b) (opcional) grade de parâmetros: parameters() %>% update() %>% grid_regular()

c) tune_grid(y ~ x + ...)

d) escolha das métricas (rmse, roc_auc, etc)

d) collect_metrics() ou autoplot() para ver o resultado

```{r}
tbsp_resamples <- vfold_cv(tbsp_train, v = 5, strata = "TB")
grid <- grid_regular(
  penalty(range = c(-4, -2)),
  levels = 20
)
```

```{r}
tbsp_lr_tune_grid <- tune_grid(
  tbsp_lr_wf,
  resamples = tbsp_resamples,
  grid = grid,
  metrics = metric_set(
    mn_log_loss, #binary cross entropy
    accuracy,
    roc_auc,
    # kap, # KAPPA
    # precision,
    # recall,
    # f_meas,
  )
)
# autoplot(tbsp_lr_tune_grid)
```

```{r}
area_rl <- collect_metrics(tbsp_lr_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  summarise(area = mean(mean),
            desvio_pad = mean(std_err))

collect_metrics(tbsp_lr_tune_grid)  |> 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_ribbon(aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.1) +
  facet_wrap(~.metric, ncol = 2, scales = "free_y") +
  scale_x_log10()
```

# Desempenho do modelo final

a) extrai melhor modelo com select_best()

b) finaliza o modelo inicial com finalize_model()

c) ajusta o modelo final com todos os dados de treino (a base de validação é incorporada)

```{r}
tbsp_lr_best_params <- select_best(tbsp_lr_tune_grid, "roc_auc")
tbsp_lr_wf <- tbsp_lr_wf |> finalize_workflow(tbsp_lr_best_params)

tbsp_lr_last_fit <- last_fit(
  tbsp_lr_wf,
  tbsp_initial_split
)

# Variáveis importantes
tbsp_lr_last_fit_model <- tbsp_lr_last_fit$.workflow[[1]]$fit$fit
vip(tbsp_lr_last_fit_model)

## 
tbsp_lr_last_fit$.metrics
```

# Guardar tudo 
```{r}
write_rds(tbsp_lr_last_fit, "data/tbsp_lr_last_fit.rds")
write_rds(tbsp_lr_model, "data/tbsp_lr_model.rds")
collect_metrics(tbsp_lr_last_fit)
tbsp_test_preds_lr <- collect_predictions(tbsp_lr_last_fit)
```

## roc

```{r}
tbsp_roc_curve_lr <- tbsp_test_preds_lr  |>  roc_curve(TB, .pred_0)
autoplot(tbsp_roc_curve_lr)

tbsp_lift_curve_lr <- tbsp_test_preds_lr  |>  lift_curve(TB, .pred_0)
autoplot(tbsp_lift_curve_lr)
```

### Matriz de Confusão
```{r}
tbsp_test_preds_lr <- tbsp_test_preds_lr |> 
  mutate(
    TB_class = factor(if_else(.pred_0 > 0.9, "0", "1"))
  ) 
tbsp_test_preds_lr |> conf_mat(TB, TB_class)
```


# Árvore de decisão

## Data prep

```{r}
tbsp_dt_recipe <- recipe(TB ~ ., data = tbsp_train |> 
                        select(-id, -cod_mun))  |> 
  step_novel(all_nominal_predictors()) |> 
  step_zv(all_predictors())
```

## Modelo

```{r}
tbsp_dt_model <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
)  |> 
  set_mode("classification")  |> 
  set_engine("rpart")
```


## Workflow

```{r}
tbsp_dt_wf <- workflow()  |> 
  add_model(tbsp_dt_model) |> 
  add_recipe(tbsp_dt_recipe)
```

## Tune
```{r}
grid_dt <- grid_random(
  cost_complexity(c(-9, -2)),
  tree_depth(range = c(5, 15)),
  min_n(range = c(20, 40)),
  size = 20
)
```


```{r}
tbsp_dt_tune_grid <- tune_grid(
  tbsp_dt_wf,
  resamples = tbsp_resamples,
  grid = grid_dt,
  metrics = metric_set(roc_auc)
)
```


```{r}
autoplot(tbsp_dt_tune_grid)
collect_metrics(tbsp_dt_tune_grid)
```

## Desempenho dos modelos finais

```{r}
tbsp_lr_best_params <- select_best(tbsp_lr_tune_grid, "roc_auc")
tbsp_lr_wf <- tbsp_lr_wf  |>  finalize_workflow(tbsp_lr_best_params)
tbsp_lr_last_fit <- last_fit(tbsp_lr_wf, tbsp_initial_split)
```


```{r}
tbsp_dt_best_params <- select_best(tbsp_dt_tune_grid, "roc_auc")
tbsp_dt_wf <- tbsp_dt_wf %>% finalize_workflow(tbsp_dt_best_params)
tbsp_dt_last_fit <- last_fit(tbsp_dt_wf, tbsp_initial_split)
```


```{r}
tbsp_test_preds <- bind_rows(
  collect_predictions(tbsp_lr_last_fit) |>  mutate(modelo = "lr"),
  collect_predictions(tbsp_dt_last_fit) |>  mutate(modelo = "dt")
)
```


```{r}
## roc
tbsp_test_preds  |> 
  group_by(modelo)  |> 
  roc_curve(TB, .pred_0)  |> 
  autoplot()
```


```{r}
## lift
tbsp_test_preds  |> 
  group_by(modelo)  |> 
  lift_curve(TB, .pred_0)  |> 
  autoplot()
```

# Variáveis importantes Regressão Logística
```{r}
tbspt_lr_last_fit_model <- tbsp_lr_last_fit$.workflow[[1]]$fit$fit
vip(tbsp_lr_last_fit_model)
```

# Árvore de Decisão
```{r}
tbsp_dt_last_fit_model <- tbsp_dt_last_fit$.workflow[[1]]$fit$fit
vip(tbsp_dt_last_fit_model)
```


```{r}
# Guardar tudo ------------------------------------------------------------

write_rds(tbsp_dt_last_fit, "tbsp_dt_last_fit.rds")
write_rds(tbsp_dt_model, "tbsp_dt_model.rds")
collect_metrics(tbsp_dt_last_fit)
collect_metrics(tbsp_lr_last_fit)

# Modelo final ------------------------------------------------------------

tbsp_final_dt_model <- tbsp_dt_wf  |>  fit(tbsp)
```

```{r}
tbsp_test_preds_dt <- collect_predictions(tbsp_dt_last_fit)
```

## roc

```{r}
tbsp_roc_curve_dt <- tbsp_test_preds_dt  |>  roc_curve(TB, .pred_0)
autoplot(tbsp_roc_curve_dt)

tbsp_lift_curve_dt <- tbsp_test_preds_dt  |>  lift_curve(TB, .pred_0)
autoplot(tbsp_lift_curve_dt)
```

### Matriz de Confusão
```{r}
tbsp_test_preds_dt <- tbsp_test_preds_dt |> 
  mutate(
    TB_class = factor(if_else(.pred_0 > 0.9, "0", "1"))
  ) 
tbsp_test_preds_dt |> conf_mat(TB, TB_class)
```

# Random Forest

## Data prep

```{r}
tbsp_rf_recipe <- recipe(TB ~ ., data = tbsp_train |> 
                        select(-id, -cod_mun))  |> 
  step_novel(all_nominal_predictors()) |> 
  step_zv(all_predictors())
```

## Modelo

```{r}
tbsp_rf_model <- rand_forest(
  min_n = tune(),
  mtry = tune(),
  trees = tune()
)  |> 
  set_mode("classification")  |> 
  set_engine("randomForest")
```


## Workflow

```{r}
tbsp_rf_wf <- workflow()  |> 
  add_model(tbsp_rf_model) |> 
  add_recipe(tbsp_rf_recipe)
```

## Tune
```{r}
grid_rf <- grid_random(
  min_n(range = c(20, 40)),
  mtry(range = c(1,10)),
  trees(range = c(1,1000) ),
  size = 20
)
```


```{r}
tbsp_rf_tune_grid <- tune_grid(
  tbsp_rf_wf,
  resamples = tbsp_resamples,
  grid = grid_rf,
  metrics = metric_set(roc_auc)
)
```


```{r}
autoplot(tbsp_rf_tune_grid)
collect_metrics(tbsp_rf_tune_grid)
```

## Desempenho dos modelos finais

```{r}
tbsp_rf_best_params <- select_best(tbsp_rf_tune_grid, "roc_auc")
tbsp_rf_wf <- tbsp_rf_wf  |>  finalize_workflow(tbsp_rf_best_params)
tbsp_rf_last_fit <- last_fit(tbsp_rf_wf, tbsp_initial_split)
```



```{r}
tbsp_test_preds <- bind_rows(
  collect_predictions(tbsp_lr_last_fit) |>  mutate(modelo = "lr"),
  collect_predictions(tbsp_rf_last_fit) |>  mutate(modelo = "rf"),
  collect_predictions(tbsp_dt_last_fit) |>  mutate(modelo = "dt")
)
```


```{r}
## roc
tbsp_test_preds  |> 
  group_by(modelo)  |> 
  roc_curve(TB, .pred_0)  |> 
  autoplot()
```


```{r}
## lift
tbsp_test_preds  |> 
  group_by(modelo)  |> 
  lift_curve(TB, .pred_0)  |> 
  autoplot()
```


# Random Forest
```{r}
tbsp_rf_last_fit_model <- tbsp_rf_last_fit$.workflow[[1]]$fit$fit
vip(tbsp_rf_last_fit_model)
```

```{r}
# Guardar tudo ------------------------------------------------------------

write_rds(tbsp_rf_last_fit, "tbsp_rf_last_fit.rds")
write_rds(tbsp_rf_model, "tbsp_rf_model.rds")
collect_metrics(tbsp_lr_last_fit)
collect_metrics(tbsp_dt_last_fit)
collect_metrics(tbsp_rf_last_fit)

# Modelo final ------------------------------------------------------------
tbsp_final_dt_model <- tbsp_rf_wf  |>  fit(tbsp)
```

```{r}
tbsp_test_preds_rf <- collect_predictions(tbsp_rf_last_fit)
```

## roc

```{r}
tbsp_roc_curve_rf <- tbsp_test_preds_rf  |>  roc_curve(TB, .pred_0)
autoplot(tbsp_roc_curve_rf)

tbsp_lift_curve_rf <- tbsp_test_preds_rf  |>  lift_curve(TB, .pred_0)
autoplot(tbsp_lift_curve_rf)
```

### Matriz de Confusão
```{r}
tbsp_test_preds_rf <- tbsp_test_preds_rf |> 
  mutate(
    TB_class = factor(if_else(.pred_0 > 0.9, "0", "1"))
  ) 
tbsp_test_preds_rf |> conf_mat(TB, TB_class)
```




# Boosting gradient tree

## Data prep

```{r}
tbsp_xgb_recipe <- recipe(TB ~ ., data = tbsp_train |> 
                        select(-id, -cod_mun))  |> 
  step_novel(all_nominal_predictors()) |> 
  step_zv(all_predictors())
```


## Estratégia de Tunagem de Hiperparâmetros

### Passo 1:

Achar uma combinação `learning_rate` e `trees` que funciona relativamente bem. 

-   `learn_rate` - 0.05, 0.1, 0.3
-   `trees` - 100, 500, 1000, 1500

## Modelo

```{r}
cores = 4
tbsp_xgb_model <- boost_tree(
  mtry = 0.8, 
  trees = tune(), # <---------------
  min_n = 5, 
  tree_depth = 4,
  loss_reduction = 0, # lambda
  learn_rate = tune(), # epsilon
  sample_size = 0.8
) |>  
  set_mode("classification")  |> 
  set_engine("xgboost", nthread = cores, counts = FALSE)
```


## Workflow

```{r}
tbsp_xgb_wf <- workflow()  |> 
  add_model(tbsp_xgb_model) |> 
  add_recipe(tbsp_xgb_recipe)
```

## Tune
```{r}
grid_xgb <- expand.grid(
  learn_rate = c(0.05, 0.3, .8, 1.2),
  trees = c(2, 250, 500, 1000)
)
```


```{r,warning=FALSE}
tbsp_xgb_tune_grid <- tune_grid(
  tbsp_xgb_wf,
  resamples = tbsp_resamples,
  grid = grid_xgb,
  metrics = metric_set(roc_auc)
)
```

#### Melhores hiperparâmetros

```{r}
autoplot(tbsp_xgb_tune_grid)
tbsp_xgb_tune_grid  |>  show_best(metric = "roc_auc", n = 6)
tbsp_xgb_select_best_passo1 <- tbsp_xgb_tune_grid %>% 
  select_best(metric = "roc_auc")
tbsp_xgb_select_best_passo1
```

### Passo 2:

São bons valores inciais. Agora, podemos tunar os parâmetros relacionados à árvore.

-   `tree_depth`: vamos deixar ele variar entre 3 e 10.
-   `min_n`: vamos deixar variar entre 5 e 90.


```{r,  cache=TRUE}
tbsp_xgb_model <- boost_tree(
  mtry = 0.8,
  trees = tbsp_xgb_select_best_passo1$trees,
  min_n = tune(),
  tree_depth = tune(), 
  loss_reduction = 0, 
  learn_rate = tbsp_xgb_select_best_passo1$learn_rate, 
  sample_size = 0.8
) %>% 
  set_mode("classification")  |> 
  set_engine("xgboost", nthread = cores, counts = FALSE)

#### Workflow
tbsp_xgb_wf <- workflow() |>  
    add_model(tbsp_xgb_model)  |>  
    add_recipe(tbsp_xgb_recipe)

#### Grid
tbsp_xgb_grid <- expand.grid(
  tree_depth = c(1, 3, 4, 6, 10), 
  min_n = c(5, 30, 60, 90, 100, 200)
)

tbsp_xgb_tune_grid <- tbsp_xgb_wf  |>  
  tune_grid(
    resamples = tbsp_resamples,
    grid = tbsp_xgb_grid,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc)
  )

#### Melhores hiperparâmetros
autoplot(tbsp_xgb_tune_grid)
tbsp_xgb_tune_grid  |>  show_best(metric = "roc_auc", n = 5)
tbsp_xgb_select_best_passo2 <- tbsp_xgb_tune_grid |>  select_best(metric = "roc_auc")
tbsp_xgb_select_best_passo2
```

### Passo 3:

Agora temos definidos:

-   `trees` = `r tbsp_xgb_select_best_passo1$trees`
-   `learn_rate` = `r tbsp_xgb_select_best_passo1$learn_rate`
-   `min_n` = `r tbsp_xgb_select_best_passo2$min_n`
-   `tree_depth` = `r tbsp_xgb_select_best_passo2$tree_depth`

Vamos então tunar o `loss_reduction`:

`loss_reduction`: vamos deixar ele variar entre 0 e 2

```{r,  cache=TRUE}
tbsp_xgb_model <- boost_tree(
  mtry = 0.8,
  trees = tbsp_xgb_select_best_passo1$trees,
  min_n = tbsp_xgb_select_best_passo2$min_n,
  tree_depth = tbsp_xgb_select_best_passo2$tree_depth, 
  loss_reduction = tune(), 
  learn_rate = tbsp_xgb_select_best_passo1$learn_rate, 
  sample_size = 0.8
) |> 
  set_mode("classification") |> 
  set_engine("xgboost", nthread = cores, counts = FALSE)

#### Workflow
tbsp_xgb_wf <- workflow() |>  
    add_model(tbsp_xgb_model)  |>  
    add_recipe(tbsp_xgb_recipe)

#### Grid
tbsp_xgb_grid <- expand.grid(
  loss_reduction = c(0, 0.05, 1, 2, 4)
)

tbsp_xgb_tune_grid <- tbsp_xgb_wf  |>  
  tune_grid(
    resamples = tbsp_resamples,
    grid = tbsp_xgb_grid,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc)
  )

#### Melhores hiperparâmetros
autoplot(tbsp_xgb_tune_grid)
tbsp_xgb_tune_grid  |>  show_best(metric = "roc_auc", n = 5)
tbsp_xgb_select_best_passo3 <- tbsp_xgb_tune_grid %>% select_best(metric = "roc_auc")
tbsp_xgb_select_best_passo3
```



### Passo 4:

Não parece que o `lossreduction` teve tanto efeito, mas, vamos usar `r tbsp_xgb_select_best_passo3$loss_reduction` que deu o melhor resultado. Até agora temos definido:

-   `trees` = `r tbsp_xgb_select_best_passo1$trees`
-   `learn_rate` = `r tbsp_xgb_select_best_passo1$learn_rate`
-   `min_n` = `r tbsp_xgb_select_best_passo2$min_n`
-   `tree_depth` = `r tbsp_xgb_select_best_passo2$tree_depth`
-   `lossreduction` = `r tbsp_xgb_select_best_passo3$loss_reduction`

Vamos então tunar o `mtry` e o `sample_size`:

-   `mtry`: de 10% a 100%
-   `sample_size`: de 50% a 100%

```{r}
tbsp_xgb_model <- boost_tree(
  mtry = tune(),
  trees = tbsp_xgb_select_best_passo1$trees,
  min_n = tbsp_xgb_select_best_passo2$min_n,
  tree_depth = tbsp_xgb_select_best_passo2$tree_depth, 
  loss_reduction = tbsp_xgb_select_best_passo3$loss_reduction, 
  learn_rate = tbsp_xgb_select_best_passo1$learn_rate, 
  sample_size = tune()
) |>  
  set_mode("classification")  |> 
  set_engine("xgboost", nthread = cores, counts = FALSE)

#### Workflow
tbsp_xgb_wf <- workflow() |>  
    add_model(tbsp_xgb_model) |>  
    add_recipe(tbsp_xgb_recipe)

#### Grid
tbsp_xgb_grid <- expand.grid(
    sample_size = seq(0.5, 1.0, length.out = 2),
    mtry = seq(0.1, 1.0, length.out = 2)
)

tbsp_xgb_tune_grid <- tbsp_xgb_wf  |>  
  tune_grid(
    resamples = tbsp_resamples,
    grid = tbsp_xgb_grid,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc)
  )

#### Melhores hiperparâmetros
autoplot(tbsp_xgb_tune_grid)
tbsp_xgb_tune_grid  |>  show_best(metric = "roc_auc", n = 5)
tbsp_xgb_select_best_passo4 <- tbsp_xgb_tune_grid  |>  select_best(metric = "roc_auc")
tbsp_xgb_select_best_passo4
```
### Passo 5:

Vimos que a melhor combinação foi

-   `mtry` = `r tbsp_xgb_select_best_passo4$mtry`
-   `sample_size` = `r tbsp_xgb_select_best_passo4$sample_size`

Agora vamos tunar o `learn_rate` e o `trees` de novo, mas deixando o `learn_rate` assumir valores menores.

```{r, warning=FALSE}
tbsp_xgb_model <- boost_tree(
  mtry = tbsp_xgb_select_best_passo4$mtry,
  trees = tune(),
  min_n = tbsp_xgb_select_best_passo2$min_n,
  tree_depth = tbsp_xgb_select_best_passo2$tree_depth, 
  loss_reduction = tbsp_xgb_select_best_passo3$loss_reduction, 
  learn_rate = tune(), 
  sample_size = tbsp_xgb_select_best_passo4$sample_size
) |> 
  set_mode("classification") |> 
  set_engine("xgboost", nthread = cores, counts = FALSE)

#### Workflow
tbsp_xgb_wf <- workflow() |>  
    add_model(tbsp_xgb_model) |>  
    add_recipe(tbsp_xgb_recipe)

#### Grid
tbsp_xgb_grid <- expand.grid(
    learn_rate = c(0.05, 0.10, 0.15, 0.25),
    trees = c(100, 250)
)

tbsp_xgb_tune_grid <- tbsp_xgb_wf  |>  
  tune_grid(
    resamples = tbsp_resamples,
    grid = tbsp_xgb_grid,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc)
  )

#### Melhores hiperparâmetros
autoplot(tbsp_xgb_tune_grid)
tbsp_xgb_tune_grid |>  show_best(metric = "roc_auc", n = 5)
tbsp_xgb_select_best_passo5 <- tbsp_xgb_tune_grid  |>  select_best(metric = "roc_auc")
tbsp_xgb_select_best_passo5
```


## Desempenho dos modelos finais

```{r, warning=FALSE}
tbsp_xgb_model <- boost_tree(
  mtry = tbsp_xgb_select_best_passo4$mtry,
  trees = tbsp_xgb_select_best_passo5$trees,
  min_n = tbsp_xgb_select_best_passo2$min_n,
  tree_depth = tbsp_xgb_select_best_passo2$tree_depth, 
  loss_reduction = tbsp_xgb_select_best_passo3$loss_reduction, 
  learn_rate = tbsp_xgb_select_best_passo5$learn_rate, 
  sample_size = tbsp_xgb_select_best_passo4$sample_size
) |> 
  set_mode("classification") |> 
  set_engine("xgboost", nthread = cores, counts = FALSE)

#### Workflow
tbsp_xgb_wf <- workflow()  |>  
    add_model(tbsp_xgb_model) |>  
    add_recipe(tbsp_xgb_recipe)

tbsp_xgb_last_fit <- tbsp_xgb_wf  |>  
  last_fit(
    split = tbsp_initial_split,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc, f_meas, accuracy, precision, recall)
  )

#### Métricas
collect_metrics(tbsp_xgb_last_fit)

#### Variáveis Importantes
tbsp_xgb_last_fit |>  
  pluck(".workflow", 1)  |>    
  extract_fit_parsnip() |>  
  vip::vip(num_features = 20)

#### Curva ROC
tbsp_xgb_last_fit  |>  
    collect_predictions()  |>  
    roc_curve(TB, .pred_0) %>% 
    autoplot()

```

## MODELO FINAL FINAL

```{r,warning=FALSE}
tbsp_xgb_modelo_final <- tbsp_xgb_wf |>  fit(tbsp)

saveRDS(tbsp_xgb_modelo_final, "tbsp_xgb_modelo_final.rds")

predict(tbsp_xgb_modelo_final, new_data = tbsp_test, type="prob")  |> 
  arrange(desc(.pred_1))

table(
  predict(tbsp_xgb_modelo_final, new_data = tbsp_test, type="prob")$.pred_1 > 0.5,
  tbsp_test$TB
)

```




```{r}
tbsp_test_preds <- bind_rows(
  collect_predictions(tbsp_lr_last_fit) |>  mutate(modelo = "lr"),
  collect_predictions(tbsp_rf_last_fit) |>  mutate(modelo = "rf"),
  collect_predictions(tbsp_dt_last_fit) |>  mutate(modelo = "dt"),
  collect_predictions(tbsp_xgb_last_fit) |>  mutate(modelo = "xgb")
)
```


```{r}
## roc
tbsp_test_preds  |> 
  group_by(modelo)  |> 
  roc_curve(TB, .pred_0)  |> 
  autoplot()
```


```{r}
## lift
tbsp_test_preds  |> 
  group_by(modelo)  |> 
  lift_curve(TB, .pred_0)  |> 
  autoplot()
```


# XGboost
```{r}
tbsp_xgb_last_fit_model <- tbsp_xgb_last_fit$.workflow[[1]]$fit$fit
vip(tbsp_xgb_last_fit_model)
```

```{r}
tbsp_test_preds_xgb <- collect_predictions(tbsp_xgb_last_fit)
```

## roc

```{r}
tbsp_roc_curve_xgb <- tbsp_test_preds_xgb  |>  roc_curve(TB, .pred_0)
autoplot(tbsp_roc_curve_xgb)

tbsp_lift_curve_xgb <- tbsp_test_preds_xgb  |>  lift_curve(TB, .pred_0)
autoplot(tbsp_lift_curve_xgb)
```

### Matriz de Confusão
```{r}
tbsp_test_preds_xgb <- tbsp_test_preds_xgb |> 
  mutate(
    TB_class = factor(if_else(.pred_0 > 0.9, "0", "1"))
  ) 
tbsp_test_preds_xgb |> conf_mat(TB, TB_class)
```


#### resumo final

```{r}
collect_metrics(tbsp_lr_last_fit)
collect_metrics(tbsp_dt_last_fit)
collect_metrics(tbsp_rf_last_fit)
collect_metrics(tbsp_xgb_last_fit)
```

```{r}
tbsp_lr_best_params
tbsp_dt_best_params
tbsp_rf_best_params
```

