---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Tuberculo Bovina

### Carregando os pacotes

```{r, warning=FALSE, error=FALSE, message=FALSE}
library(tidymodels)
library(ISLR)
library(tidyverse)
library(modeldata)
library(pROC)
library(vip)
library(readxl)
```

### Entrada de dados
```{r}
tbsp <- read_excel("data-raw/TBSP_outubro.xls") |> 
  mutate(TB = forcats::as_factor(TB))
glimpse(tbsp)
tbsp |> count(TB)
```


### Definindo a base de treino e a base de teste

```{r}
set.seed(1)
tbsp_initial_split <- initial_split(tbsp, strata = "TB", prop = 0.75)

tbsp_train <- training(tbsp_initial_split)
tbsp_train |> count(TB)
tbsp_test  <- testing(tbsp_initial_split)
tbsp_test |> count(TB)
```


### Análise exploratória dos dados

```{r}
skimr::skim(tbsp_train)
visdat::vis_miss(tbsp_train)
# tbsp_train  |> 
#   select(-id, -cod_mun)  |> 
#   cor( method = "spearman")  |> 
#   corrplot::corrplot()
```
# Data Prep

```{r}
tbsp_recipe <- recipe(TB ~ ., data = tbsp_train |> 
                        select(-id, -cod_mun)
                      ) |> 
  step_normalize(all_numeric_predictors())  |> 
  step_novel(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  # step_poly(all_numeric_predictors(), degree = 9)  |> 
  step_dummy(all_nominal_predictors())

bake(prep(tbsp_recipe), new_data = NULL)
visdat::vis_miss(bake(prep(tbsp_recipe), new_data = NULL))
```


### Modelo  

#### Definição de

a) a f(x): logistc_reg()

b) modo (natureza da var resp): classification

c) hiperparametros a tunar: penalty = tune()

d) hiperparametros  não tunado: mixture = 1 # LASSO

e) o motor que queremos usar: glmnet


```{r}
tbsp_lr_model <- logistic_reg(penalty = tune(), mixture = 1)  |> 
  set_mode("classification") |> 
  set_engine("glmnet")
```
  
# Workflow

```{r}
tbsp_wf <- workflow()  |> 
  add_model(tbsp_lr_model) %>%
  add_recipe(tbsp_recipe)
```

## Tunagem (tune)

a) bases de reamostragem para validação: vfold_cv()

b) (opcional) grade de parâmetros: parameters() %>% update() %>% grid_regular()

c) tune_grid(y ~ x + ...)

d) escolha das métricas (rmse, roc_auc, etc)

d) collect_metrics() ou autoplot() para ver o resultado

```{r}
tbsp_resamples <- vfold_cv(tbsp_train, v = 5, strata = "TB")
grid <- grid_regular(
  penalty(range = c(-4, -1)),
  levels = 20
)
```

```{r}
tbsp_lr_tune_grid <- tune_grid(
  tbsp_wf,
  resamples = tbsp_resamples,
  grid = grid,
  metrics = metric_set(
    mn_log_loss, #binary cross entropy
    accuracy,
    roc_auc,
    # kap, # KAPPA
    precision,
    # recall,
    # f_meas,
  )
)
# autoplot(tbsp_lr_tune_grid)
```

```{r}
collect_metrics(tbsp_lr_tune_grid)

collect_metrics(tbsp_lr_tune_grid)  |> 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_ribbon(aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.1) +
  facet_wrap(~.metric, ncol = 2, scales = "free_y") +
  scale_x_log10()
```

# Desempenho do modelo final

a) extrai melhor modelo com select_best()

b) finaliza o modelo inicial com finalize_model()

c) ajusta o modelo final com todos os dados de treino (a base de validação é incorporada)

```{r}
tbsp_lr_best_params <- select_best(tbsp_lr_tune_grid, "roc_auc")
tbsp_wf <- tbsp_wf |> finalize_workflow(tbsp_lr_best_params)

tbsp_lr_last_fit <- last_fit(
  tbsp_wf,
  tbsp_initial_split
)

# Variáveis importantes
tbsp_lr_last_fit_model <- tbsp_lr_last_fit$.workflow[[1]]$fit$fit
vip(tbsp_lr_last_fit_model)
```

# Guardar tudo 
```{r}
write_rds(tbsp_lr_last_fit, "data/tbsp_lr_last_fit.rds")
write_rds(tbsp_lr_model, "data/tbsp_lr_model.rds")
collect_metrics(tbsp_lr_last_fit)
tbsp_test_preds <- collect_predictions(tbsp_lr_last_fit)
```

## roc

```{r}

tbsp_roc_curve <- tbsp_test_preds  |>  roc_curve(TB, .pred_0)
autoplot(tbsp_roc_curve)

tbsp_lift_curve <- tbsp_test_preds  |>  lift_curve(TB, .pred_0)
autoplot(tbsp_lift_curve)
```

### Matriz de Confusão
```{r}
tbsp_test_preds <- tbsp_test_preds |> 
  mutate(
    TB_class = factor(if_else(.pred_0 > 0.9, "0", "1"))
  ) 
levels(tbsp_test_preds$ TB)
levels(tbsp_test_preds$ TB_class)
tbsp_test_preds |> conf_mat(TB, TB_class)
```
## risco por faixa de TB
```{r}
tbsp_test_preds  |> 
  mutate(
    tb =  factor(ntile(.pred_0, 10))
  ) |> 
  count(tb, TB) %>%
  ggplot(aes(x = tb, y = n, fill = TB)) +
  geom_col(position = "fill") +
  geom_label(aes(label = n), position = "fill") +
  coord_flip()
```


## gráfico sobre os da classe 1

```{r}
percentis = 20
tbsp_test_preds |> 
  mutate(
    tb = factor(ntile(.pred_0, percentis))
  ) |> 
  filter(TB == 1)  |> 
  group_by(tb, .drop = FALSE)  |> 
  summarise(
    n = n(),
    media = mean(.pred_0)
  ) |> 
  mutate(p = n/sum(n)) |> 
  ggplot(aes(x = p, y = tb)) +
  geom_col() +
  geom_label(aes(label = scales::percent(p))) +
  geom_vline(xintercept = 1/percentis, colour = "red", linetype = "dashed", size = 1)
```


